{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dbdd76d",
   "metadata": {},
   "source": [
    "# TabPFN Inference Pipeline for Behaviour Classification\n",
    "\n",
    "This notebook runs inference using a trained TabPFN model on new, unlabeled accelerometer data.  \n",
    "We use the [TabPFN model](../models/tabpfn_b4_basal/) trained on Burst 4 basal corrected data.  \n",
    "The [tabpfn environment](../environment_tabpfn.yml) is required to run this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cb90189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d53e7aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tabpfn_model(model_dir, model_name):\n",
    "    \"\"\"\n",
    "    Load a TabPFN model and its label encoder from disk.\n",
    "    Returns: model, label_encoder\n",
    "    \"\"\"\n",
    "    model_path = f\"{model_dir}/{model_name}_tabpfn_model.joblib\"\n",
    "    encoder_path = f\"{model_dir}/{model_name}_label_encoder.joblib\"\n",
    "    model = joblib.load(model_path)\n",
    "    label_encoder = joblib.load(encoder_path)\n",
    "    print(f\"Loaded TabPFN model from {model_path}\")\n",
    "    print(f\"Loaded label encoder from {encoder_path}\")\n",
    "    return model, label_encoder\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# model, label_encoder = load_tabpfn_model(\"../models/tabpfn_b1_unc\", \"tabpfn_b1_unc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d625cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pytz\n",
    "import datetime\n",
    "\n",
    "\n",
    "def extract_features_for_inference(pq_file_path, date, chunk_size=10000):\n",
    "    \"\"\"\n",
    "    Given a parquet file and a date, extract all feature data for that date and yield (features, metadata) chunks.\n",
    "    Assumes 'burst_start_time' is the date column, and 'Ind_ID', 'new_burst', 'burst_id' are metadata columns.\n",
    "    \"\"\"\n",
    "    # Set timezone\n",
    "    tz = pytz.timezone(\"Africa/Johannesburg\")\n",
    "    # Create timezone-aware datetime objects for start and end of day\n",
    "    start_dt = tz.localize(datetime.datetime.combine(date, datetime.time(0, 0, 0)))\n",
    "    end_dt = tz.localize(datetime.datetime.combine(date, datetime.time(23, 59, 59)))\n",
    "\n",
    "    # Read filtered data\n",
    "    df = pq.read_table(\n",
    "        pq_file_path,\n",
    "        filters=[\n",
    "            (\"burst_start_time\", \">=\", start_dt),\n",
    "            (\"burst_start_time\", \"<\", end_dt),\n",
    "        ],\n",
    "    ).to_pandas()\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"No data found for {date}\")\n",
    "        return\n",
    "\n",
    "    # Identify feature columns (exclude metadata)\n",
    "    exclude_cols = [\"Ind_ID\", \"new_burst\", \"burst_id\", \"burst_start_time\"]\n",
    "    feature_columns = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "    # Sort for consistency\n",
    "    df = df.sort_values(by=[\"Ind_ID\", \"burst_id\", \"new_burst\"]).reset_index(drop=True)\n",
    "\n",
    "    n = len(df)\n",
    "\n",
    "    chunks = []\n",
    "    for start in range(0, n, chunk_size):\n",
    "        chunk = df.iloc[start : start + chunk_size]\n",
    "        features = chunk[feature_columns].copy()\n",
    "        metadata = chunk[[\"Ind_ID\", \"burst_id\", \"new_burst\", \"burst_start_time\"]].copy()\n",
    "        chunks.append((features, metadata))\n",
    "\n",
    "    print(f\"Extracted {n} rows for {date} and created {len(chunks)} chunks.\")\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# pq_file_path = (\n",
    "#     \"../data/raw/focal_sampling/focal_sampled_features_burst_1_uncorrected.parquet\"\n",
    "# )\n",
    "# date = datetime.date(2022, 7, 3)\n",
    "# chunks = extract_features_for_inference(pq_file_path, date)\n",
    "# features_df_1, metadata_df_1 = chunks[0]\n",
    "# print(features_df_1.head())\n",
    "# print(metadata_df_1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88ef5b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_format_probas_tabpfn(model, label_encoder, chunks):\n",
    "    \"\"\"\n",
    "    For each chunk (features, metadata), run TabPFN prediction, format the probas DataFrame, and combine all chunks.\n",
    "    Returns a single DataFrame with per-class probabilities, predicted behaviour, and metadata.\n",
    "    \"\"\"\n",
    "    probas_list = []\n",
    "    class_names = list(label_encoder.classes_)\n",
    "\n",
    "    for features_df, metadata_df in chunks:\n",
    "        # Predict probabilities and class indices\n",
    "        probas = model.predict_proba(features_df)\n",
    "        preds = np.argmax(probas, axis=1)\n",
    "        # Format probabilities DataFrame\n",
    "        probas_df = pd.DataFrame(probas, columns=class_names)\n",
    "        probas_df[\"predicted_behaviour\"] = label_encoder.inverse_transform(preds)\n",
    "        # Combine with metadata\n",
    "        combined = pd.concat(\n",
    "            [metadata_df.reset_index(drop=True), probas_df.reset_index(drop=True)],\n",
    "            axis=1,\n",
    "        )\n",
    "        probas_list.append(combined)\n",
    "\n",
    "    # Concatenate all chunks\n",
    "    final_df = pd.concat(probas_list, ignore_index=True)\n",
    "    # Reorder columns if needed\n",
    "    col_order = [\n",
    "        \"Ind_ID\",\n",
    "        \"burst_id\",\n",
    "        \"new_burst\",\n",
    "        \"burst_start_time\",\n",
    "        \"predicted_behaviour\",\n",
    "    ]\n",
    "    class_cols = [col for col in class_names if col in final_df.columns]\n",
    "    final_df = final_df[\n",
    "        col_order\n",
    "        + class_cols\n",
    "        + [col for col in final_df.columns if col not in col_order + class_cols]\n",
    "    ]\n",
    "    return final_df\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# final_df = predict_and_format_probas_tabpfn(model, label_encoder, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d3be530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict_dates_tabpfn(\n",
    "    pq_file_path,\n",
    "    model_dir,\n",
    "    model_name,\n",
    "    output_parquet=None,\n",
    "    schema=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    For each unique date in the parquet file, run TabPFN inference and concatenate results.\n",
    "    Args:\n",
    "        pq_file_path: Path to input parquet file\n",
    "        model_dir: Directory containing model files\n",
    "        model_name: Model name (used for loading)\n",
    "        output_parquet: If provided, saves the concatenated DataFrame to this path\n",
    "        schema: pyarrow schema for output parquet\n",
    "    Returns:\n",
    "        pd.DataFrame with predictions for all dates (if output_parquet is None)\n",
    "    \"\"\"\n",
    "    # Load model and label encoder\n",
    "    model, label_encoder = load_tabpfn_model(model_dir, model_name)\n",
    "\n",
    "    # Get all unique dates in the parquet file\n",
    "    df = pd.read_parquet(pq_file_path, columns=[\"burst_start_time\"])\n",
    "    df[\"date\"] = pd.to_datetime(df[\"burst_start_time\"]).dt.date\n",
    "    unique_dates = sorted(df[\"date\"].unique())\n",
    "    print(f\"Found {len(unique_dates)} unique dates in the data.\")\n",
    "    del df\n",
    "\n",
    "    probas_list = []\n",
    "    for date in tqdm(unique_dates, desc=\"Processing dates\"):\n",
    "        chunks = extract_features_for_inference(pq_file_path, date)\n",
    "        if not chunks:\n",
    "            continue\n",
    "        probas = predict_and_format_probas_tabpfn(model, label_encoder, chunks)\n",
    "        probas_list.append(probas)\n",
    "\n",
    "    final_df = pd.concat(probas_list, ignore_index=True)\n",
    "\n",
    "    # Ensure correct filetype\n",
    "    final_df[\"Ind_ID\"] = final_df[\"Ind_ID\"].astype(str)\n",
    "    final_df[\"burst_id\"] = final_df[\"burst_id\"].astype(int)\n",
    "    final_df[\"new_burst\"] = final_df[\"new_burst\"].astype(int)\n",
    "    final_df[\"predicted_behaviour\"] = final_df[\"predicted_behaviour\"].astype(str)\n",
    "    for col in label_encoder.classes_:\n",
    "        if col in final_df.columns:\n",
    "            final_df[col] = final_df[col].astype(float).round(4)\n",
    "\n",
    "    # Ensure column order\n",
    "    class_cols = [\n",
    "        \"Eating\",\n",
    "        \"Grooming actor\",\n",
    "        \"Grooming receiver\",\n",
    "        \"Resting\",\n",
    "        \"Running\",\n",
    "        \"Self-scratching\",\n",
    "        \"Sleeping\",\n",
    "        \"Walking\",\n",
    "    ]\n",
    "    final_df = final_df.reindex(\n",
    "        columns=[\n",
    "            \"Ind_ID\",\n",
    "            \"burst_id\",\n",
    "            \"new_burst\",\n",
    "            \"burst_start_time\",\n",
    "            \"predicted_behaviour\",\n",
    "        ]\n",
    "        + class_cols\n",
    "    )\n",
    "\n",
    "    if output_parquet:\n",
    "        table = pa.Table.from_pandas(final_df, schema=schema, preserve_index=False)\n",
    "        pq.write_table(table, output_parquet, compression=\"zstd\", compression_level=9)\n",
    "        print(f\"Saved predictions to {output_parquet}\")\n",
    "        return None\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037cf4f3",
   "metadata": {},
   "source": [
    "## Run inference\n",
    "We run inference on the focal sampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09aacc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for output parquet file\n",
    "parquet_schema = pa.schema(\n",
    "    [\n",
    "        (\"Ind_ID\", pa.string()),\n",
    "        (\"burst_id\", pa.int32()),\n",
    "        (\"new_burst\", pa.int32()),\n",
    "        (\"burst_start_time\", pa.timestamp(\"ns\", tz=\"Africa/Johannesburg\")),\n",
    "        (\"predicted_behaviour\", pa.string()),\n",
    "        (\"Eating\", pa.float32()),\n",
    "        (\"Grooming actor\", pa.float32()),\n",
    "        (\"Grooming receiver\", pa.float32()),\n",
    "        (\"Resting\", pa.float32()),\n",
    "        (\"Running\", pa.float32()),\n",
    "        (\"Self-scratching\", pa.float32()),\n",
    "        (\"Sleeping\", pa.float32()),\n",
    "        (\"Walking\", pa.float32()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a85a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1022106, 64)\n",
      "Columns: ['Ind_ID', 'burst_id', 'new_burst', 'burst_start_time', 'X_mean', 'Y_mean', 'Z_mean', 'X_sd', 'Y_sd', 'Z_sd', 'X_min', 'Y_min', 'Z_min', 'X_max', 'Y_max', 'Z_max', 'X_range', 'Y_range', 'Z_range', 'X_var', 'Y_var', 'Z_var', 'X_inv_coef_var', 'Y_inv_coef_var', 'Z_inv_coef_var', 'X_skewness', 'Y_skewness', 'Z_skewness', 'X_kurtosis', 'Y_kurtosis', 'Z_kurtosis', 'q', 'X_dps_mean', 'X_dps2_mean', 'X_fdps_mean', 'X_fdps2_mean', 'Y_dps_mean', 'Y_dps2_mean', 'Y_fdps_mean', 'Y_fdps2_mean', 'Z_dps_mean', 'Z_dps2_mean', 'Z_fdps_mean', 'Z_fdps2_mean', 'mean_vedba', 'mean_vedbas', 'mean_ODBA', 'X_PDBA_mean', 'Y_PDBA_mean', 'Z_PDBA_mean', 'X_static_mean', 'Y_static_mean', 'Z_static_mean', 'pitch_angle', 'roll_angle', 'PC1_var', 'PC2_var', 'PC3_var', 'PC1_mean', 'PC2_mean', 'PC3_mean', 'PC1_PC2_ratio', 'PC1_PC3_ratio', 'PC2_PC3_ratio']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\n",
    "    \"../data/raw/focal_sampling/features/focal_sampled_features_burst_2_uncorrected.parquet\"\n",
    ")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3374f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect warnings\n",
    "collected_warnings = []\n",
    "\n",
    "\n",
    "def custom_warning_handler(message, category, filename, lineno, file=None, line=None):\n",
    "    collected_warnings.append(f\"{category.__name__}: {message} ({filename}:{lineno})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab76f696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TabPFN model from ../models/tabpfn_b4_basal//tabpfn_b4_basal_tabpfn_model.joblib\n",
      "Loaded label encoder from ../models/tabpfn_b4_basal//tabpfn_b4_basal_label_encoder.joblib\n",
      "Found 21 unique dates in the data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 103575 rows for 2022-07-03 and created 11 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:   5%|▍         | 1/21 [02:04<41:36, 124.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 103580 rows for 2022-07-10 and created 11 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:  10%|▉         | 2/21 [04:08<39:20, 124.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 103456 rows for 2022-07-14 and created 11 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:  14%|█▍        | 3/21 [06:09<36:52, 122.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 103572 rows for 2022-07-15 and created 11 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:  19%|█▉        | 4/21 [08:13<34:54, 123.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 103588 rows for 2022-07-18 and created 11 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:  24%|██▍       | 5/21 [10:17<32:55, 123.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 103576 rows for 2022-07-19 and created 11 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:  29%|██▊       | 6/21 [12:21<30:53, 123.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 103624 rows for 2022-07-31 and created 11 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:  33%|███▎      | 7/21 [14:18<28:21, 121.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 103660 rows for 2022-09-03 and created 11 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:  38%|███▊      | 8/21 [16:21<26:26, 122.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 103640 rows for 2022-09-08 and created 11 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:  43%|████▎     | 9/21 [18:04<23:12, 116.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 98888 rows for 2022-09-09 and created 10 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:  48%|████▊     | 10/21 [19:41<20:12, 110.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 103628 rows for 2022-09-19 and created 11 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:  52%|█████▏    | 11/21 [21:28<18:10, 109.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 103663 rows for 2022-09-25 and created 11 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:  57%|█████▋    | 12/21 [23:18<16:25, 109.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 103624 rows for 2022-09-27 and created 11 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:  62%|██████▏   | 13/21 [25:06<14:31, 108.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 103644 rows for 2022-09-28 and created 11 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:  67%|██████▋   | 14/21 [26:57<12:47, 109.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 85912 rows for 2023-06-05 and created 9 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:  71%|███████▏  | 15/21 [28:32<10:30, 105.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 85372 rows for 2023-06-11 and created 9 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:  76%|███████▌  | 16/21 [30:06<08:28, 101.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 85252 rows for 2023-06-14 and created 9 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:  81%|████████  | 17/21 [31:38<06:36, 99.00s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 86508 rows for 2023-06-20 and created 9 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:  86%|████████▌ | 18/21 [33:10<04:50, 96.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 84964 rows for 2023-06-22 and created 9 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:  90%|█████████ | 19/21 [34:46<03:12, 96.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 85464 rows for 2023-06-28 and created 9 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:  95%|█████████▌| 20/21 [36:21<01:36, 96.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 85024 rows for 2023-06-30 and created 9 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates: 100%|██████████| 21/21 [37:59<00:00, 108.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to ../data/output/inference_results/tabpfn_b4_basal_focal_sampled_random_predictions.parquet\n"
     ]
    }
   ],
   "source": [
    "# TabPFN Burst 4 Rotbasal\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"always\")\n",
    "    warnings.showwarning = custom_warning_handler\n",
    "\n",
    "    batch_predict_dates_tabpfn(\n",
    "        pq_file_path=\"../data/raw/focal_sampling/features/focal_sampled_features_burst_4_rotbasal.parquet\",\n",
    "        model_dir=\"../models/tabpfn_b4_basal/\",\n",
    "        model_name=\"tabpfn_b4_basal\",\n",
    "        output_parquet=\"../data/output/inference_results/tabpfn_b4_basal_focal_sampled_random_predictions.parquet\",\n",
    "        schema=parquet_schema,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabpfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
