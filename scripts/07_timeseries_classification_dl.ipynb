{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioural classification from accelerometer timeseries using DL models\n",
    "\n",
    "This script is used to run three classifiers: [LSTM](https://timeseriesai.github.io/tsai/models.rnn.html#lstm), [TSSequencer](https://timeseriesai.github.io/tsai/models.tssequencerplus.html) and [HydraMultiROCKET](https://timeseriesai.github.io/tsai/models.hydramultirocketplus.html) using the tsai package.  \n",
    "Create the [tsai environment](../environment_tsai.yml) and use this for running this script.  \n",
    "The functions are set up to run classifications on both multiclass (8 behavioural classes) and binary (activity/inactivity) target. See the [behavioural attribution notebook](03_combine_burst_attributions.ipynb) for details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# For optimisation\n",
    "# import wandb\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# from tsai.all import *\n",
    "import tsai.all as ts\n",
    "from fastai.interpret import ClassificationInterpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_acc_parquet(pq_file_path, binary_col, multiclass_col):\n",
    "    \"\"\"Load and preprocess accelerometer data from a parquet file with annotations.\n",
    "\n",
    "    This function loads accelerometer data from a parquet file, merges it with\n",
    "    annotation data, and performs basic cleaning operations such as removing\n",
    "    rows marked for removal.\n",
    "\n",
    "    Args:\n",
    "        pq_file_path (str): Path to the parquet file containing accelerometer data.\n",
    "        binary_col (str): Column name for binary classification (e.g., \"activity\").\n",
    "        multiclass_col (str): Column name for multiclass classification (e.g., \"attribution_merged\").\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned dataframe containing accelerometer data merged with annotations.\n",
    "    \"\"\"\n",
    "    # Load parquet file\n",
    "    acc = pd.read_parquet(pq_file_path)\n",
    "\n",
    "    # Load annotations\n",
    "    annotations = pd.read_csv(\n",
    "        \"../data/temp/annotations/attributions_merged_majority_outliers.csv\"\n",
    "    )\n",
    "\n",
    "    # Identify burst from parquet file name\n",
    "    pq_file_name = os.path.basename(pq_file_path)\n",
    "    burst = pq_file_name.split(\"_\")[4]\n",
    "    burst = \"burst_\" + burst\n",
    "\n",
    "    # Filter annotations for the current burst\n",
    "    annotations = annotations[annotations[\"burst\"] == burst]\n",
    "\n",
    "    # Join annotations with accelerometer data\n",
    "    acc = acc.merge(annotations, on=[\"Ind_ID\", \"new_burst\"], how=\"left\")\n",
    "\n",
    "    # Remove other string columns except features and IDs\n",
    "    acc = acc.drop(\n",
    "        columns=acc.select_dtypes(include=\"object\").columns.difference(\n",
    "            [binary_col, multiclass_col, \"feature\", \"id\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Remove rows with \"Remove\" value\n",
    "    # First obtain rows with remove in the binary and multiclass columns\n",
    "    binary_remove = acc[acc[binary_col] == \"Remove\"].index.sort_values()\n",
    "    multi_remove = acc[acc[multiclass_col] == \"Remove\"].index.sort_values()\n",
    "    # Check if both are equal:\n",
    "    are_equal = binary_remove.equals(multi_remove)\n",
    "    if are_equal:\n",
    "        # Drop rows with remove in both columns\n",
    "        acc = acc.drop(index=binary_remove)\n",
    "        print(\"Removed rows with 'Remove' in both classification columns\")\n",
    "    else:\n",
    "        print(\"Remove indices are not equal, please check the data\")\n",
    "\n",
    "    # Return the cleaned dataframe\n",
    "    return acc\n",
    "\n",
    "\n",
    "# Test function\n",
    "# pq_file_path = \"../data/temp/acc_tsai/Burst_1/annotated_acc_tsai_burst_1_uncorrected.parquet\"\n",
    "# acc = load_acc_parquet(pq_file_path, \"activity\", \"attribution_merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dsets(acc, random_seed, behav_attribution_column, cols_to_drop=None):\n",
    "    \"\"\"Create a TSDatasets object from a DataFrame for time series classification.\n",
    "\n",
    "    This function takes accelerometer data in a DataFrame format, processes it\n",
    "    and converts it into a TSDatasets object suitable for training time series\n",
    "    classification models with tsai. It handles splitting data into train/validation\n",
    "    sets, categorizing labels, and type conversion.\n",
    "\n",
    "    Args:\n",
    "        acc (pd.DataFrame): DataFrame containing accelerometer data and labels.\n",
    "        random_seed (int): Random seed for reproducible train/test splits.\n",
    "        behav_attribution_column (str): Column name containing the target labels.\n",
    "        cols_to_drop (list, optional): Columns to remove from the DataFrame before processing.\n",
    "\n",
    "    Returns:\n",
    "        TSDatasets: A dataset object ready for use with tsai DataLoaders.\n",
    "    \"\"\"\n",
    "    # Ensure cols_to_drop is a list and add 'new_burst' if it exists in the dataframe\n",
    "    if cols_to_drop is None:\n",
    "        cols_to_drop = []\n",
    "    elif not isinstance(cols_to_drop, list):\n",
    "        cols_to_drop = [cols_to_drop]\n",
    "    all_cols_to_drop = cols_to_drop.copy()\n",
    "    if \"new_burst\" in acc.columns:\n",
    "        all_cols_to_drop.append(\"new_burst\")\n",
    "\n",
    "    X, y = ts.df2xy(\n",
    "        acc.drop(columns=all_cols_to_drop),\n",
    "        sample_col=\"id\",\n",
    "        feat_col=\"feature\",\n",
    "        target_col=behav_attribution_column,\n",
    "        data_cols=None,\n",
    "    )\n",
    "    # Print shape of x andy\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    # Split into train and test\n",
    "    splits = ts.get_splits(\n",
    "        y,\n",
    "        valid_size=0.2,\n",
    "        stratify=True,\n",
    "        random_state=random_seed,\n",
    "        shuffle=True,\n",
    "        show_plot=False,\n",
    "    )\n",
    "\n",
    "    # Function to ensure labels are categorised and only one column is chosen\n",
    "    # def y_func(o):\n",
    "    #     return o[\n",
    "    #         :, 0\n",
    "    #     ].astype(\n",
    "    #         \"<U20\"\n",
    "    #     )  # Only use first column, since labels are the same across the multivariate dataset\n",
    "    def y_func(o):\n",
    "        # Convert to string\n",
    "        labels = o[:, 0].astype(\"<U20\")\n",
    "\n",
    "        # If binary task with \"Active\"/\"Inactive\", ensure \"Active\" is encoded as 1\n",
    "        unique_labels = np.unique(labels)\n",
    "        if (\n",
    "            len(unique_labels) == 2\n",
    "            and \"Active\" in unique_labels\n",
    "            and \"Inactive\" in unique_labels\n",
    "        ):\n",
    "            # Map \"Inactive\" to 0, \"Active\" to 1\n",
    "            return np.array([0 if label == \"Inactive\" else 1 for label in labels])\n",
    "\n",
    "        return labels\n",
    "\n",
    "    # Convert X to float\n",
    "    X_mod = X.astype(np.float64)\n",
    "\n",
    "    # Create datasets for use with dataloaders\n",
    "    tfms = [None, [ts.Categorize()]]\n",
    "    dsets = ts.TSDatasets(X_mod, y=y_func(y), tfms=tfms, splits=splits, inplace=True)\n",
    "\n",
    "    return dsets\n",
    "\n",
    "\n",
    "# Test function\n",
    "# Test function\n",
    "# acc_dset = create_dsets(\n",
    "#     acc, 42, \"attribution_merged\", cols_to_drop=[\"activity\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models_on_dataset(dset, target_type=\"binary\"):\n",
    "    \"\"\"Run architecture comparison on a dataset\n",
    "\n",
    "    Args:\n",
    "        dset: TSDataset object\n",
    "        target_type: Type of classification (\"binary\" or \"multiclass\")\n",
    "\n",
    "    Returns:\n",
    "        tuple: (metrics_df, confusion_df)\n",
    "    \"\"\"\n",
    "    # Create dataloaders\n",
    "    dls = ts.TSDataLoaders.from_dsets(dset.train, dset.valid, bs=[64, 128])\n",
    "\n",
    "    # Determine metrics\n",
    "    is_binary = len(dls.vocab) == 2\n",
    "    roc_metric = ts.RocAucBinary() if is_binary else ts.RocAuc()\n",
    "    metrics = [\n",
    "        ts.accuracy,\n",
    "        roc_metric,\n",
    "        ts.BalancedAccuracy(),\n",
    "        ts.F1Score(average=\"weighted\"),\n",
    "        ts.Precision(average=\"weighted\"),\n",
    "        ts.Recall(average=\"weighted\"),\n",
    "    ]\n",
    "\n",
    "    # Define architectures to test\n",
    "    archs = [\n",
    "        (ts.HydraMultiRocket, {}),\n",
    "        (ts.LSTM, {\"n_layers\": 6, \"bidirectional\": True}),\n",
    "        (ts.TSSequencer, {}),\n",
    "        # Add more as needed\n",
    "    ]\n",
    "\n",
    "    # Initialize results collections\n",
    "    results = []\n",
    "    confusion_data = []\n",
    "\n",
    "    # Run each architecture\n",
    "    for arch, params in archs:\n",
    "        try:\n",
    "            model = ts.create_model(arch, dls=dls, **params)\n",
    "            learn = ts.Learner(dls, model, metrics=metrics)\n",
    "\n",
    "            # Train model\n",
    "            start = time.time()\n",
    "            lr_max = learn.lr_find()\n",
    "            learn.fit_one_cycle(50, lr_max)\n",
    "            elapsed = time.time() - start\n",
    "\n",
    "            # Get metrics\n",
    "            vals = learn.recorder.values[-1]\n",
    "\n",
    "            # Store metrics\n",
    "            results.append(\n",
    "                {\n",
    "                    \"model\": arch.__name__,\n",
    "                    \"params\": str(params),\n",
    "                    \"train_loss\": vals[0],\n",
    "                    \"valid_loss\": vals[1],\n",
    "                    \"accuracy\": vals[2],\n",
    "                    \"roc_auc\": vals[3],\n",
    "                    \"balanced_acc\": vals[4],\n",
    "                    \"f1_score\": vals[5],\n",
    "                    \"precision\": vals[6],\n",
    "                    \"recall\": vals[7],\n",
    "                    \"time\": int(elapsed),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Process confusion matrix\n",
    "            interp = ClassificationInterpretation.from_learner(learn)\n",
    "            conf_matrix = interp.confusion_matrix()\n",
    "            classes = dset.vocab\n",
    "\n",
    "            for actual_idx, actual in enumerate(classes):\n",
    "                for pred_idx, predicted in enumerate(classes):\n",
    "                    confusion_data.append(\n",
    "                        {\n",
    "                            \"model\": arch.__name__,\n",
    "                            \"actual_label\": actual,\n",
    "                            \"predicted_label\": predicted,\n",
    "                            \"count\": conf_matrix[actual_idx, pred_idx],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {arch.__name__}: {str(e)}\")\n",
    "\n",
    "    # Make dataframes\n",
    "    results_df = pd.DataFrame(results)\n",
    "    confusion_df = pd.DataFrame(confusion_data)\n",
    "\n",
    "    # For binary confusion matrix replace 0/1 with labels\n",
    "    if target_type == \"binary\":\n",
    "        confusion_df[\"actual_label\"] = confusion_df[\"actual_label\"].replace(\n",
    "            {0: \"Inactive\", 1: \"Active\"}\n",
    "        )\n",
    "        confusion_df[\"predicted_label\"] = confusion_df[\"predicted_label\"].replace(\n",
    "            {0: \"Inactive\", 1: \"Active\"}\n",
    "        )\n",
    "\n",
    "    return results_df, confusion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_parquet_file(file_path, random_seed_list, binary_col, multiclass_col):\n",
    "    \"\"\"Process a single parquet file with multiple seeds\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to parquet file\n",
    "        random_seed_list: List of random seeds\n",
    "\n",
    "    Returns:\n",
    "        dict: Dict with all results DataFrames\n",
    "    \"\"\"\n",
    "    # Extract metadata from path\n",
    "    burst = os.path.basename(os.path.dirname(file_path))  # from folder\n",
    "    correction_type = (\n",
    "        os.path.basename(file_path).split(\"_\")[-1].split(\".\")[0]\n",
    "    )  # from filename\n",
    "    print(f\"Processing {correction_type} for {burst}\")\n",
    "\n",
    "    # Initialize result collections\n",
    "    binary_metrics_list = []\n",
    "    binary_conf_list = []\n",
    "    multiclass_metrics_list = []\n",
    "    multiclass_conf_list = []\n",
    "\n",
    "    # Process each seed\n",
    "    for seed in random_seed_list:\n",
    "        print(f\"  Processing with seed {seed}\")\n",
    "\n",
    "        # Load data\n",
    "        acc_data = load_acc_parquet(file_path, binary_col, multiclass_col)\n",
    "\n",
    "        # Create datasets\n",
    "        binary_dset = create_dsets(\n",
    "            acc_data, seed, binary_col, cols_to_drop=[multiclass_col]\n",
    "        )\n",
    "        multiclass_dset = create_dsets(\n",
    "            acc_data, seed, multiclass_col, cols_to_drop=[binary_col]\n",
    "        )\n",
    "\n",
    "        # Run models on binary data\n",
    "        binary_metrics, binary_conf = run_models_on_dataset(binary_dset, \"binary\")\n",
    "\n",
    "        # Add metadata\n",
    "        binary_metrics[\"burst\"] = burst\n",
    "        binary_metrics[\"correction_type\"] = correction_type\n",
    "        binary_metrics[\"random_seed\"] = seed\n",
    "        binary_metrics[\"target\"] = \"Activity\"\n",
    "\n",
    "        binary_conf[\"burst\"] = burst\n",
    "        binary_conf[\"correction_type\"] = correction_type\n",
    "        binary_conf[\"random_seed\"] = seed\n",
    "        binary_conf[\"target\"] = \"Activity\"\n",
    "\n",
    "        # Run models on multiclass data\n",
    "        multiclass_metrics, multiclass_conf = run_models_on_dataset(\n",
    "            multiclass_dset, \"multiclass\"\n",
    "        )\n",
    "\n",
    "        # Add metadata\n",
    "        multiclass_metrics[\"burst\"] = burst\n",
    "        multiclass_metrics[\"correction_type\"] = correction_type\n",
    "        multiclass_metrics[\"random_seed\"] = seed\n",
    "        multiclass_metrics[\"target\"] = \"Behaviour\"\n",
    "\n",
    "        multiclass_conf[\"burst\"] = burst\n",
    "        multiclass_conf[\"correction_type\"] = correction_type\n",
    "        multiclass_conf[\"random_seed\"] = seed\n",
    "        multiclass_conf[\"target\"] = \"Behaviour\"\n",
    "\n",
    "        # Append to result lists\n",
    "        binary_metrics_list.append(binary_metrics)\n",
    "        binary_conf_list.append(binary_conf)\n",
    "        multiclass_metrics_list.append(multiclass_metrics)\n",
    "        multiclass_conf_list.append(multiclass_conf)\n",
    "\n",
    "    # Define the common columns to appear first\n",
    "    first_columns = [\"burst\", \"correction_type\", \"target\", \"random_seed\"]\n",
    "\n",
    "    # Concatenate and reorder binary metrics\n",
    "    binary_metrics_df = pd.concat(binary_metrics_list)\n",
    "    if not binary_metrics_df.empty:\n",
    "        remaining_cols = [\n",
    "            col for col in binary_metrics_df.columns if col not in first_columns\n",
    "        ]\n",
    "        binary_metrics_df = binary_metrics_df[first_columns + remaining_cols]\n",
    "\n",
    "    # Concatenate and reorder binary confusion matrix\n",
    "    binary_conf_df = pd.concat(binary_conf_list)\n",
    "    if not binary_conf_df.empty:\n",
    "        remaining_cols = [\n",
    "            col for col in binary_conf_df.columns if col not in first_columns\n",
    "        ]\n",
    "        binary_conf_df = binary_conf_df[first_columns + remaining_cols]\n",
    "\n",
    "    # Concatenate and reorder multiclass metrics\n",
    "    multiclass_metrics_df = pd.concat(multiclass_metrics_list)\n",
    "    if not multiclass_metrics_df.empty:\n",
    "        remaining_cols = [\n",
    "            col for col in multiclass_metrics_df.columns if col not in first_columns\n",
    "        ]\n",
    "        multiclass_metrics_df = multiclass_metrics_df[first_columns + remaining_cols]\n",
    "\n",
    "    # Concatenate and reorder multiclass confusion matrix\n",
    "    multiclass_conf_df = pd.concat(multiclass_conf_list)\n",
    "    if not multiclass_conf_df.empty:\n",
    "        remaining_cols = [\n",
    "            col for col in multiclass_conf_df.columns if col not in first_columns\n",
    "        ]\n",
    "        multiclass_conf_df = multiclass_conf_df[first_columns + remaining_cols]\n",
    "\n",
    "    # Return reordered DataFrames\n",
    "    return {\n",
    "        \"binary_metrics\": binary_metrics_df,\n",
    "        \"binary_conf\": binary_conf_df,\n",
    "        \"multiclass_metrics\": multiclass_metrics_df,\n",
    "        \"multiclass_conf\": multiclass_conf_df,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_comparisons(folder_path, random_seed_list, correction_filters=None):\n",
    "    \"\"\"Run model comparisons across multiple parquet files\n",
    "\n",
    "    Args:\n",
    "        folder_path: Path to folder containing parquet files\n",
    "        random_seed_list: List of random seeds\n",
    "        max_files: Maximum files to process (for testing)\n",
    "\n",
    "    Returns:\n",
    "        dict: Dict with combined results DataFrames\n",
    "    \"\"\"\n",
    "    # Find all parquet files\n",
    "    all_files = glob.glob(os.path.join(folder_path, \"**/*.parquet\"), recursive=True)\n",
    "    # Apply correction type filters if provided\n",
    "    if correction_filters is not None:\n",
    "        all_files = [\n",
    "            f\n",
    "            for f in all_files\n",
    "            if any(corr_type in os.path.basename(f) for corr_type in correction_filters)\n",
    "        ]\n",
    "\n",
    "    print(f\"Found {len(all_files)} parquet files\")\n",
    "\n",
    "    # Process all files and collect results\n",
    "    all_results = []\n",
    "    for file_path in all_files:\n",
    "        result = process_parquet_file(\n",
    "            file_path, random_seed_list, \"activity\", \"attribution_merged\"\n",
    "        )\n",
    "        all_results.append(result)\n",
    "\n",
    "    # Combine results\n",
    "    combined_results = {\n",
    "        \"binary_metrics\": pd.concat([r[\"binary_metrics\"] for r in all_results]),\n",
    "        \"binary_conf\": pd.concat([r[\"binary_conf\"] for r in all_results]),\n",
    "        \"multiclass_metrics\": pd.concat([r[\"multiclass_metrics\"] for r in all_results]),\n",
    "        \"multiclass_conf\": pd.concat([r[\"multiclass_conf\"] for r in all_results]),\n",
    "    }\n",
    "\n",
    "    return combined_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test functions sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DL training started for Burst_4\n",
      "['../data/temp/acc_tsai/Burst_4\\\\annotated_acc_tsai_burst_4_uncorrected.parquet']\n",
      "['uncorrected']\n"
     ]
    }
   ],
   "source": [
    "# Test loading files\n",
    "folder_location = \"../data/temp/acc_tsai/Burst_4/\"\n",
    "\n",
    "burst = folder_location.split(\"/\")[-2]\n",
    "print(\"DL training started for\", burst)\n",
    "correction_types_to_process = [\"uncorrected\", \"rotdaily\", \"rotbasal\"]\n",
    "\n",
    "# Find all parquet files\n",
    "all_files = glob.glob(os.path.join(folder_location, \"**/*.parquet\"), recursive=True)\n",
    "# Apply correction type filters if provided\n",
    "if correction_types_to_process is not None:\n",
    "    all_files = [\n",
    "        f\n",
    "        for f in all_files\n",
    "        if any(\n",
    "            corr_type in os.path.basename(f)\n",
    "            for corr_type in correction_types_to_process\n",
    "        )\n",
    "    ]\n",
    "\n",
    "print(all_files)\n",
    "\n",
    "filetypes = [os.path.basename(f).split(\"_\")[-1].split(\".\")[0] for f in all_files]\n",
    "print(filetypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for one file\n",
    "pq_file_path = (\n",
    "    \"../data/temp/acc_tsai/Burst_1/annotated_acc_tsai_burst_1_uncorrected.parquet\"\n",
    ")\n",
    "random_seed_list = [42]\n",
    "b1_unc = process_parquet_file(\n",
    "    pq_file_path, random_seed_list, \"activity\", \"attribution_merged\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_unc_bin_met = b1_unc[\"binary_metrics\"]\n",
    "b1_unc_beh_met = b1_unc[\"multiclass_metrics\"]\n",
    "b1_unc_bin_conf = b1_unc[\"binary_conf\"]\n",
    "b1_unc_beh_conf = b1_unc[\"multiclass_conf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run function over multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_folder_locations = [\n",
    "    \"../data/temp/acc_tsai/Burst_1/\",\n",
    "    \"../data/temp/acc_tsai/Burst_2/\",\n",
    "    \"../data/temp/acc_tsai/Burst_3/\",\n",
    "    \"../data/temp/acc_tsai/Burst_4/\",\n",
    "]\n",
    "# Random seed list\n",
    "random_seeds = [42, 100, 123, 1234, 123456]\n",
    "correction_types_to_process = [\"uncorrected\", \"rotdaily\", \"rotbasal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_burst_results = [\n",
    "    run_model_comparisons(folder, random_seeds, correction_types_to_process)\n",
    "    for folder in pq_folder_locations\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain individual dataframes for results and confusion matrices\n",
    "binary_metrics_list = [result[\"binary_metrics\"] for result in all_burst_results]\n",
    "binary_conf_list = [result[\"binary_conf\"] for result in all_burst_results]\n",
    "multiclass_metrics_list = [result[\"multiclass_metrics\"] for result in all_burst_results]\n",
    "multiclass_conf_list = [result[\"multiclass_conf\"] for result in all_burst_results]\n",
    "\n",
    "# Concatenate all results into single dataframes\n",
    "binary_metrics = pd.concat(binary_metrics_list, ignore_index=True)\n",
    "binary_conf = pd.concat(binary_conf_list, ignore_index=True)\n",
    "multiclass_metrics = pd.concat(multiclass_metrics_list, ignore_index=True)\n",
    "multiclass_conf = pd.concat(multiclass_conf_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to CSV files\n",
    "# binary_metrics.to_csv(\n",
    "#     \"../data/output/activity_comparison/activity_acceleration_dl_metrics.csv\",\n",
    "#     index=False,\n",
    "# )\n",
    "# binary_conf.to_csv(\n",
    "#     \"../data/output/activity_comparison/activity_acceleration_dl_confusion.csv\",\n",
    "#     index=False,\n",
    "# )\n",
    "multiclass_metrics.to_csv(\n",
    "    \"../data/output/behaviour_comparison/behaviour_acceleration_dl_metrics.csv\",\n",
    "    index=False,\n",
    ")\n",
    "multiclass_conf.to_csv(\n",
    "    \"../data/output/behaviour_comparison/behaviour_acceleration_dl_confusion.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
