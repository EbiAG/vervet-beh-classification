{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioural classification from features using DL models\n",
    "\n",
    "This script is used to run two classifiers: [Category Embedding](https://pytorch-tabular.readthedocs.io/en/latest/models/#category-embedding-model-multi-layer-perceptron) and [GANDALF](https://pytorch-tabular.readthedocs.io/en/latest/models/#gated-adaptive-network-for-deep-automated-learning-of-features-gandalf) using the torch tabular package.  \n",
    "Create the [torch_tabular environment](../environment_torch_tabular.yml) and use this for running this script.  \n",
    "The functions are set up to run classifications on both multiclass (8 behavioural classes) and binary (activity/inactivity) target. See the [behavioural attribution notebook](03_combine_burst_attributions.ipynb) for details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.config import (\n",
    "    DataConfig,\n",
    "    OptimizerConfig,\n",
    "    TrainerConfig,\n",
    ")\n",
    "from pytorch_tabular.models import (\n",
    "    CategoryEmbeddingModelConfig,\n",
    "    GANDALFConfig,\n",
    ")\n",
    "from pytorch_tabular.models.common.heads import LinearHeadConfig\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features_parquet(pq_file_path, binary_col, multiclass_col):\n",
    "    \"\"\"Load and preprocess feature data from a parquet file with annotations.\n",
    "\n",
    "    Args:\n",
    "        pq_file_path (str): Path to the parquet file containing feature data.\n",
    "        binary_col (str): Column name for binary classification (e.g., \"activity\").\n",
    "        multiclass_col (str): Column name for multiclass classification (e.g., \"attribution_merged\").\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned dataframe containing features merged with annotations.\n",
    "    \"\"\"\n",
    "    # Load parquet file\n",
    "    features = pd.read_parquet(pq_file_path)\n",
    "\n",
    "    # Load annotations\n",
    "    annotations = pd.read_csv(\n",
    "        \"../data/temp/annotations/attributions_merged_majority_outliers.csv\"\n",
    "    )\n",
    "\n",
    "    # Identify burst from parquet file name\n",
    "    pq_file_name = os.path.basename(pq_file_path)\n",
    "    burst = pq_file_name.split(\"_\")[3]\n",
    "    burst = \"burst_\" + burst\n",
    "    print(burst)\n",
    "\n",
    "    # Filter annotations for the current burst\n",
    "    annotations = annotations[annotations[\"burst\"] == burst]\n",
    "\n",
    "    # Join annotations with feature data\n",
    "    features = features.merge(annotations, on=[\"Ind_ID\", \"new_burst\"], how=\"left\")\n",
    "\n",
    "    # Remove other string columns except features and IDs\n",
    "    features = features.drop(\n",
    "        columns=features.select_dtypes(include=\"object\").columns.difference(\n",
    "            [binary_col, multiclass_col, \"Ind_ID\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Remove rows with \"Remove\" value\n",
    "    # First obtain rows with remove in the binary and multiclass columns\n",
    "    binary_remove = features[features[binary_col] == \"Remove\"].index.sort_values()\n",
    "    multi_remove = features[features[multiclass_col] == \"Remove\"].index.sort_values()\n",
    "    # Check if both are equal:\n",
    "    are_equal = binary_remove.equals(multi_remove)\n",
    "    if are_equal:\n",
    "        # Drop rows with remove in both columns\n",
    "        features = features.drop(index=binary_remove)\n",
    "        print(\"Removed rows with 'Remove' in both classification columns\")\n",
    "    else:\n",
    "        print(\"Remove indices are not equal, please check the data\")\n",
    "\n",
    "    # Return the cleaned dataframe\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_dl_models(X, y, random_seed, is_binary=False):\n",
    "    \"\"\"\n",
    "    Train DL models and evaluate using scikit-learn metrics AFTER training.\n",
    "    \"\"\"\n",
    "    # --- Data Preparation ---\n",
    "    target_encoder = LabelEncoder()\n",
    "    y_encoded = target_encoder.fit_transform(y)\n",
    "    num_classes = len(target_encoder.classes_)\n",
    "\n",
    "    if is_binary and num_classes == 2:\n",
    "        if (\n",
    "            \"Active\" in target_encoder.classes_\n",
    "            and \"Inactive\" in target_encoder.classes_\n",
    "        ):\n",
    "            if target_encoder.transform([\"Active\"])[0] != 1:\n",
    "                target_encoder.classes_ = target_encoder.classes_[::-1]\n",
    "                y_encoded = target_encoder.transform(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_encoded, stratify=y_encoded, test_size=0.25, random_state=random_seed\n",
    "    )\n",
    "\n",
    "    # Verify number of classes in train and test match\n",
    "    if len(np.unique(y_train)) != num_classes or len(np.unique(y_test)) != num_classes:\n",
    "        raise ValueError(\n",
    "            f\"Mismatch in number of classes: Train classes {np.unique(y_train)}, Test classes {np.unique(y_test)}\"\n",
    "        )\n",
    "\n",
    "    train_df = X_train.copy()\n",
    "    test_df = X_test.copy()\n",
    "    train_df[\"target\"] = y_train\n",
    "    test_df[\"target\"] = y_test\n",
    "    train_df.reset_index(drop=True, inplace=True)\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # PyTorch Tabular Configurations\n",
    "    # Data config\n",
    "    data_config = DataConfig(\n",
    "        target=[\"target\"],\n",
    "        continuous_cols=list(X_train.columns),\n",
    "        categorical_cols=[],\n",
    "        pin_memory=True,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    \n",
    "    trainer_config = TrainerConfig(\n",
    "        auto_lr_find=True,\n",
    "        batch_size=64,\n",
    "        max_epochs=100,\n",
    "        # min_epochs=50,\n",
    "        early_stopping=\"valid_loss\",\n",
    "        early_stopping_mode=\"min\",\n",
    "        early_stopping_min_delta=0.001,\n",
    "        early_stopping_patience=10,\n",
    "        checkpoints=\"valid_loss\",\n",
    "        load_best=True,\n",
    "        track_grad_norm=2,\n",
    "        progress_bar=\"none\",\n",
    "        accelerator=\"gpu\",\n",
    "    )\n",
    "\n",
    "    # Head config\n",
    "    head_config = LinearHeadConfig(\n",
    "        layers=\"\",\n",
    "        dropout=0.1,\n",
    "        initialization=\"kaiming\",\n",
    "    ).__dict__\n",
    "\n",
    "    # Optimizer config\n",
    "    optimizer_config = OptimizerConfig(\n",
    "        lr_scheduler=\"CosineAnnealingWarmRestarts\",\n",
    "        lr_scheduler_params={\"T_0\": 100, \"T_mult\": 1, \"eta_min\": 1e-5},\n",
    "    )\n",
    "\n",
    "    # Create model list with set of common parameters\n",
    "    common_params = {\n",
    "        \"task\": \"classification\",\n",
    "        \"head\": \"LinearHead\",\n",
    "        \"head_config\": head_config,\n",
    "    }\n",
    "\n",
    "    # Define models to test with specific configurations\n",
    "    model_list = [\n",
    "        CategoryEmbeddingModelConfig(layers=\"1024-512-256\", **common_params),\n",
    "        GANDALFConfig(gflu_stages=15, learnable_sparsity=False, **common_params),\n",
    "    ]\n",
    "\n",
    "    # Model training loop\n",
    "    metrics_results = []\n",
    "    confusion_data = []  # If you need confusion matrix data later\n",
    "\n",
    "    for model_config in model_list:\n",
    "        model_name = model_config.__class__.__name__.replace(\"Config\", \"\")\n",
    "\n",
    "        try:\n",
    "            print(f\"  Training {model_name}\")\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Create model instance\n",
    "            model = TabularModel(\n",
    "                data_config=data_config,\n",
    "                model_config=model_config,\n",
    "                optimizer_config=optimizer_config,\n",
    "                trainer_config=trainer_config,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            model.fit(train=train_df, validation=test_df)\n",
    "\n",
    "            # Get predictions and probabilities\n",
    "            pred_df_with_proba = model.predict(test_df)\n",
    "\n",
    "            # Extract class predictions\n",
    "            y_pred = pred_df_with_proba[\"prediction\"].values\n",
    "\n",
    "            # Extract probabilities based on task type\n",
    "            roc_auc = np.nan  # Default to NaN\n",
    "            proba_values = None\n",
    "\n",
    "            # Extract columns with probabilities for different classes\n",
    "            prob_cols = [\n",
    "                col for col in pred_df_with_proba.columns if \"_probability\" in col\n",
    "            ]\n",
    "\n",
    "            if not prob_cols:\n",
    "                print(\n",
    "                    f\"Warning: Probability columns not found for {model_name}. Cannot calculate ROC AUC.\"\n",
    "                )\n",
    "            else:\n",
    "                # Sort columns numerically by class index (0_probability, 1_probability, ...)\n",
    "                prob_cols.sort(key=lambda name: int(name.split(\"_\")[0]))\n",
    "\n",
    "                proba_df = pred_df_with_proba[prob_cols]\n",
    "                proba_values = proba_df.values\n",
    "\n",
    "                # Calculate ROC AUC using extracted probabilities\n",
    "                try:\n",
    "                    if is_binary:\n",
    "                        # Expect columns like '0_probability', '1_probability'\n",
    "                        if proba_values.shape[1] >= 2:\n",
    "                            # Use probability of the positive class (index 1, after sorting)\n",
    "                            positive_proba = proba_values[:, 1]\n",
    "                            roc_auc = roc_auc_score(y_test, positive_proba)\n",
    "                        elif proba_values.shape[1] == 1:\n",
    "                            # Handle case where only one probability (e.g., positive class) might be returned\n",
    "                            roc_auc = roc_auc_score(y_test, proba_values[:, 0])\n",
    "                        else:\n",
    "                            print(\n",
    "                                f\"Warning: Unexpected probability shape for binary task in {model_name}: {proba_values.shape}\"\n",
    "                            )\n",
    "                    else:  # Multiclass\n",
    "                        if proba_values.shape[1] == num_classes:\n",
    "                            roc_auc = roc_auc_score(\n",
    "                                y_test,\n",
    "                                proba_values,\n",
    "                                multi_class=\"ovr\",\n",
    "                                average=\"weighted\",\n",
    "                            )\n",
    "                        else:\n",
    "                            print(\n",
    "                                f\"Warning: Probability shape mismatch for multiclass task in {model_name}. Expected {num_classes} columns, got {proba_values.shape[1]}.\"\n",
    "                            )\n",
    "\n",
    "                except ValueError as e:\n",
    "                    print(f\"Could not calculate ROC AUC for {model_name}: {e}\")\n",
    "                    roc_auc = np.nan  # Set back to NaN if calculation fails\n",
    "\n",
    "            # Calculate other metrics using y_pred\n",
    "            if y_pred.size > 0:\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "                f1 = f1_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "                precision = precision_score(\n",
    "                    y_test, y_pred, average=\"weighted\", zero_division=0\n",
    "                )\n",
    "                recall = recall_score(\n",
    "                    y_test, y_pred, average=\"weighted\", zero_division=0\n",
    "                )\n",
    "\n",
    "                # Calculate and Store Confusion Matrix\n",
    "                cm = confusion_matrix(\n",
    "                    y_test, y_pred, labels=np.arange(num_classes)\n",
    "                )  # Ensure all classes are included\n",
    "                for i in range(len(cm)):\n",
    "                    for j in range(len(cm)):\n",
    "                        confusion_data.append(\n",
    "                            {\n",
    "                                \"model\": model_name,\n",
    "                                \"actual_label\": i,\n",
    "                                \"predicted_label\": j,\n",
    "                                \"count\": cm[i][j],\n",
    "                            }\n",
    "                        )\n",
    "            else:\n",
    "                # Assign NaN if prediction failed\n",
    "                accuracy, balanced_acc, f1, precision, recall = [np.nan] * 5\n",
    "\n",
    "            train_time = time.time() - start_time\n",
    "\n",
    "            # Store results\n",
    "            metrics_results.append(\n",
    "                {\n",
    "                    \"model\": model_name,\n",
    "                    \"roc_auc\": roc_auc,\n",
    "                    \"accuracy\": accuracy,  # Simple accuracy\n",
    "                    \"balanced_acc\": balanced_acc,  # Use this instead of renaming later\n",
    "                    \"f1_score\": f1,\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"time\": train_time,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Cleanup\n",
    "            del model\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"!! Error training/evaluating {model_name}: {str(e)}\")\n",
    "            print(traceback.format_exc())\n",
    "            # Append placeholder results if needed\n",
    "            metrics_results.append(\n",
    "                {\n",
    "                    \"model\": model_name,\n",
    "                    \"roc_auc\": np.nan,\n",
    "                    \"accuracy\": np.nan,\n",
    "                    \"balanced_acc\": np.nan,\n",
    "                    \"f1_score\": np.nan,\n",
    "                    \"precision\": np.nan,\n",
    "                    \"recall\": np.nan,\n",
    "                    \"time\": np.nan,\n",
    "                    \"random_seed\": random_seed,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    metrics_df = pd.DataFrame(metrics_results)\n",
    "    confusion_df = pd.DataFrame(confusion_data)\n",
    "\n",
    "    # Map the encoded labels back to original classes if possible\n",
    "    original_classes = target_encoder.classes_\n",
    "    if len(confusion_df) > 0:\n",
    "        confusion_df[\"actual_label\"] = confusion_df[\"actual_label\"].apply(\n",
    "            lambda x: original_classes[x] if x < len(original_classes) else x\n",
    "        )\n",
    "        confusion_df[\"predicted_label\"] = confusion_df[\"predicted_label\"].apply(\n",
    "            lambda x: original_classes[x] if x < len(original_classes) else x\n",
    "        )\n",
    "\n",
    "    return metrics_df, confusion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_parquet_file(\n",
    "    file_path,\n",
    "    random_seed_list,\n",
    "    binary_col=\"activity\",\n",
    "    multiclass_col=\"attribution_merged\",\n",
    "):\n",
    "    \"\"\"Process a single parquet file with both binary and multiclass classification using DL models.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the parquet file\n",
    "        random_seed_list (list): List of random seeds to use\n",
    "        binary_col (str): Column name for binary classification\n",
    "        multiclass_col (str): Column name for multiclass classification\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing all results DataFrames\n",
    "    \"\"\"\n",
    "    # Extract metadata from path\n",
    "    burst = os.path.basename(os.path.dirname(file_path))\n",
    "    correction_type = os.path.basename(file_path).split(\"_\")[-1].split(\".\")[0]\n",
    "    print(f\"Processing {correction_type} for {burst}\")\n",
    "\n",
    "    # Initialize result collections\n",
    "    binary_metrics_list = []\n",
    "    binary_conf_list = []\n",
    "    multiclass_metrics_list = []\n",
    "    multiclass_conf_list = []\n",
    "\n",
    "    # Load feature data once\n",
    "    features_df = load_features_parquet(file_path, binary_col, multiclass_col)\n",
    "\n",
    "    # Create feature matrix X and target variables just once - outside the loop\n",
    "    X = features_df.drop(\n",
    "        [binary_col, multiclass_col, \"new_burst\", \"Ind_ID\"], axis=1, errors=\"ignore\"\n",
    "    )\n",
    "    y_binary = features_df[binary_col]\n",
    "    y_multiclass = features_df[multiclass_col]\n",
    "\n",
    "    # Process each seed\n",
    "    for seed in random_seed_list:\n",
    "        print(f\"  Processing with seed {seed}\")\n",
    "\n",
    "        # Binary classification\n",
    "        print(f\"    Running binary classification ({binary_col})\")\n",
    "        binary_metrics, binary_conf = features_dl_models(\n",
    "            X, y_binary, seed, is_binary=True\n",
    "        )\n",
    "\n",
    "        # Add metadata to binary results\n",
    "        binary_metrics[\"correction_type\"] = correction_type\n",
    "        binary_metrics[\"burst\"] = burst\n",
    "        binary_metrics[\"target\"] = \"Activity\"\n",
    "        binary_metrics[\"random_seed\"] = seed\n",
    "\n",
    "        binary_conf[\"correction_type\"] = correction_type\n",
    "        binary_conf[\"burst\"] = burst\n",
    "        binary_conf[\"target\"] = \"Activity\"\n",
    "        binary_conf[\"random_seed\"] = seed\n",
    "\n",
    "        # Multiclass classification\n",
    "        print(f\"    Running multiclass classification ({multiclass_col})\")\n",
    "        multiclass_metrics, multiclass_conf = features_dl_models(\n",
    "            X, y_multiclass, seed, is_binary=False\n",
    "        )\n",
    "\n",
    "        # Add metadata to multiclass results\n",
    "        multiclass_metrics[\"correction_type\"] = correction_type\n",
    "        multiclass_metrics[\"burst\"] = burst\n",
    "        multiclass_metrics[\"target\"] = \"Behaviour\"\n",
    "        multiclass_metrics[\"random_seed\"] = seed\n",
    "\n",
    "        multiclass_conf[\"correction_type\"] = correction_type\n",
    "        multiclass_conf[\"burst\"] = burst\n",
    "        multiclass_conf[\"target\"] = \"Behaviour\"\n",
    "        multiclass_conf[\"random_seed\"] = seed\n",
    "\n",
    "        # Append to result lists\n",
    "        binary_metrics_list.append(binary_metrics)\n",
    "        binary_conf_list.append(binary_conf)\n",
    "        multiclass_metrics_list.append(multiclass_metrics)\n",
    "        multiclass_conf_list.append(multiclass_conf)\n",
    "\n",
    "    # Combine results\n",
    "    all_binary_metrics = (\n",
    "        pd.concat(binary_metrics_list) if binary_metrics_list else pd.DataFrame()\n",
    "    )\n",
    "    all_binary_conf = (\n",
    "        pd.concat(binary_conf_list) if binary_conf_list else pd.DataFrame()\n",
    "    )\n",
    "    all_multiclass_metrics = (\n",
    "        pd.concat(multiclass_metrics_list)\n",
    "        if multiclass_metrics_list\n",
    "        else pd.DataFrame()\n",
    "    )\n",
    "    all_multiclass_conf = (\n",
    "        pd.concat(multiclass_conf_list) if multiclass_conf_list else pd.DataFrame()\n",
    "    )\n",
    "\n",
    "    # Standard column order for metrics\n",
    "    metric_columns = [\n",
    "        \"burst\",\n",
    "        \"correction_type\",\n",
    "        \"target\",\n",
    "        \"random_seed\",\n",
    "        \"model\",\n",
    "        \"roc_auc\",\n",
    "        \"accuracy\",\n",
    "        \"balanced_acc\",\n",
    "        \"f1_score\",\n",
    "        \"precision\",\n",
    "        \"recall\",\n",
    "        \"time\",\n",
    "    ]\n",
    "\n",
    "    # Standard column order for confusion matrices\n",
    "    conf_columns = [\n",
    "        \"burst\",\n",
    "        \"correction_type\",\n",
    "        \"target\",\n",
    "        \"random_seed\",\n",
    "        \"model\",\n",
    "        \"actual_label\",\n",
    "        \"predicted_label\",\n",
    "        \"count\",\n",
    "    ]\n",
    "\n",
    "    # Reorder columns\n",
    "    if not all_binary_metrics.empty:\n",
    "        all_binary_metrics = all_binary_metrics[metric_columns]\n",
    "    if not all_binary_conf.empty:\n",
    "        all_binary_conf = all_binary_conf[conf_columns]\n",
    "    if not all_multiclass_metrics.empty:\n",
    "        all_multiclass_metrics = all_multiclass_metrics[metric_columns]\n",
    "    if not all_multiclass_conf.empty:\n",
    "        all_multiclass_conf = all_multiclass_conf[conf_columns]\n",
    "\n",
    "    # Create result dict before cleanup\n",
    "    result_dict = {\n",
    "        \"binary_metrics\": all_binary_metrics,\n",
    "        \"binary_conf\": all_binary_conf,\n",
    "        \"multiclass_metrics\": all_multiclass_metrics,\n",
    "        \"multiclass_conf\": all_multiclass_conf,\n",
    "    }\n",
    "\n",
    "    # Force garbage collection to free memory\n",
    "    gc.collect()\n",
    "\n",
    "    # Empty PyTorch CUDA cache if available\n",
    "    try:\n",
    "        import torch\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    except (ImportError, AttributeError):\n",
    "        pass\n",
    "\n",
    "    # Return the result dictionary\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_run_feat_dl(\n",
    "    folder_location,\n",
    "    random_seed_list,\n",
    "    binary_col=\"activity\",\n",
    "    multiclass_col=\"attribution_merged\",\n",
    "    correction_filters=None,\n",
    "):\n",
    "    \"\"\"Process all parquet files in a folder with multiple random seeds.\n",
    "\n",
    "    Args:\n",
    "        folder_location (str): Path to folder containing parquet files\n",
    "        random_seed_list (list): List of random seeds to use\n",
    "        binary_col (str): Column name for binary classification\n",
    "        multiclass_col (str): Column name for multiclass classification\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with combined results DataFrames\n",
    "    \"\"\"\n",
    "    filetype = folder_location.split(\"/\")[-2]\n",
    "    print(f\"DL training started for {filetype}\")\n",
    "\n",
    "    # Find all parquet files\n",
    "    all_files = [\n",
    "        os.path.normpath(f).replace(\"\\\\\", \"/\")\n",
    "        for f in glob.glob(folder_location + \"**/*.parquet\", recursive=True)\n",
    "    ]\n",
    "\n",
    "    # Apply correction type filters if provided\n",
    "    if correction_filters is not None:\n",
    "        all_files = [\n",
    "            f\n",
    "            for f in all_files\n",
    "            if any(corr_type in os.path.basename(f) for corr_type in correction_filters)\n",
    "        ]\n",
    "\n",
    "    print(f\"Found {len(all_files)} parquet files\")\n",
    "\n",
    "    # Process all files and collect results\n",
    "    all_results = []\n",
    "    for file_path in all_files:\n",
    "        result = process_parquet_file(\n",
    "            file_path, random_seed_list, binary_col, multiclass_col\n",
    "        )\n",
    "        all_results.append(result)\n",
    "\n",
    "    # Combine results from all files\n",
    "    combined_results = {\n",
    "        \"binary_metrics\": pd.concat(\n",
    "            [r[\"binary_metrics\"] for r in all_results if not r[\"binary_metrics\"].empty]\n",
    "        ),\n",
    "        \"binary_conf\": pd.concat(\n",
    "            [r[\"binary_conf\"] for r in all_results if not r[\"binary_conf\"].empty]\n",
    "        ),\n",
    "        \"multiclass_metrics\": pd.concat(\n",
    "            [\n",
    "                r[\"multiclass_metrics\"]\n",
    "                for r in all_results\n",
    "                if not r[\"multiclass_metrics\"].empty\n",
    "            ]\n",
    "        ),\n",
    "        \"multiclass_conf\": pd.concat(\n",
    "            [\n",
    "                r[\"multiclass_conf\"]\n",
    "                for r in all_results\n",
    "                if not r[\"multiclass_conf\"].empty\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    return combined_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test functions sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_file_path = \"../data/raw/features/Burst_2/annotated_features_burst_2_RB.parquet\"\n",
    "random_seed_list = [42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_feat = load_features_parquet(\n",
    "    pq_file_path, binary_col=\"activity\", multiclass_col=\"attribution_merged\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_col = \"activity\"\n",
    "multiclass_col = \"attribution_merged\"\n",
    "\n",
    "# Initialize result collections\n",
    "binary_metrics_list = []\n",
    "binary_conf_list = []\n",
    "multiclass_metrics_list = []\n",
    "multiclass_conf_list = []\n",
    "\n",
    "# Create feature matrix X and target variables just once - outside the loop\n",
    "X = acc_feat.drop(\n",
    "    [binary_col, multiclass_col, \"new_burst\", \"Ind_ID\"], axis=1, errors=\"ignore\"\n",
    ")\n",
    "y_binary = acc_feat[binary_col]\n",
    "y_multiclass = acc_feat[multiclass_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_metrics, multi_conf = features_dl_models(\n",
    "    X, y_multiclass, random_seed_list[0], is_binary=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_sweep_df = features_dl_models(X, y_binary, random_seed_list[0], is_binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_met = process_parquet_file(\n",
    "    pq_file_path,\n",
    "    random_seed_list,\n",
    "    binary_col=\"activity\",\n",
    "    multiclass_col=\"attribution_merged\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_met_bin = acc_met[\"binary_metrics\"]\n",
    "acc_met_multi = acc_met[\"multiclass_metrics\"]\n",
    "acc_conf_bin = acc_met[\"binary_conf\"]\n",
    "acc_conf_multi = acc_met[\"multiclass_conf\"]\n",
    "\n",
    "# Concatenate the results\n",
    "acc_met_bin = pd.concat([acc_met_bin])\n",
    "acc_met_multi = pd.concat([acc_met_multi])\n",
    "acc_conf_bin = pd.concat([acc_conf_bin])\n",
    "acc_conf_multi = pd.concat([acc_conf_multi])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run over whole folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seeds = [42, 100, 123, 1234, 123456]\n",
    "# random_seeds = [42]\n",
    "correction_types_to_process = [\"uncorrected\", \"rotdaily\", \"rotbasal\"]\n",
    "folder_locations = [\n",
    "    \"../data/raw/features/Burst_1/\",\n",
    "    \"../data/raw/features/Burst_2/\",\n",
    "    \"../data/raw/features/Burst_3/\",\n",
    "    \"../data/raw/features/Burst_4/\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the ML training for all files in the specified folders\n",
    "all_burst_results = [\n",
    "    wrap_run_feat_dl(\n",
    "        folder, random_seeds, correction_filters=correction_types_to_process\n",
    "    )\n",
    "    for folder in folder_locations\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain individual dataframes for results and confusion matrices\n",
    "binary_metrics_list = [result[\"binary_metrics\"] for result in all_burst_results]\n",
    "binary_conf_list = [result[\"binary_conf\"] for result in all_burst_results]\n",
    "multiclass_metrics_list = [result[\"multiclass_metrics\"] for result in all_burst_results]\n",
    "multiclass_conf_list = [result[\"multiclass_conf\"] for result in all_burst_results]\n",
    "\n",
    "# Concatenate all results into single dataframes\n",
    "binary_metrics = pd.concat(binary_metrics_list, ignore_index=True)\n",
    "binary_conf = pd.concat(binary_conf_list, ignore_index=True)\n",
    "multiclass_metrics = pd.concat(multiclass_metrics_list, ignore_index=True)\n",
    "multiclass_conf = pd.concat(multiclass_conf_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to CSV files\n",
    "# binary_metrics.to_csv(\n",
    "#     \"../data/output/activity_comparison/activity_features_dl_metrics.csv\",\n",
    "#     index=False,\n",
    "# )\n",
    "# binary_conf.to_csv(\n",
    "#     \"../data/output/activity_comparison/activity_features_dl_confusion.csv\",\n",
    "#     index=False,\n",
    "# )\n",
    "multiclass_metrics.to_csv(\n",
    "    \"../data/output/behaviour_comparison/behaviour_features_dl_metrics.csv\",\n",
    "    index=False,\n",
    ")\n",
    "multiclass_conf.to_csv(\n",
    "    \"../data/output/behaviour_comparison/behaviour_features_dl_confusion.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_tabular",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
